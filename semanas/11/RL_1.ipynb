{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of Reinformcent Learnig \n",
    "\n",
    "Based on Examples of datacamp\n",
    "<!--<img src=\"imagen.png\">-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enviroment anacond3 \n",
    "\n",
    "C:\\swig\\swigwin-421 descargue e instale en el "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network initialized as:\n",
      " Network(\n",
      "  (linear): Linear(in_features=8, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import gym \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# iniatiate the lunar lander enviroment\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, dim_imputs, dim_outputs):\n",
    "        super(Network, self).__init__()\n",
    "        #define a linear trasnformation layer\n",
    "        self.linear = nn.Linear(dim_imputs, dim_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "            return self.linear(x)\n",
    "\n",
    "\n",
    "#dim_inputs = env.observation_space.shape[0]\n",
    "#dim_outputs = env.action_space.n\n",
    "\n",
    "dim_inputs = 8\n",
    "dim_outputs = 4\n",
    "\n",
    "network = Network(dim_inputs, dim_outputs)\n",
    "\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.0001)\n",
    "print(\"Network initialized as:\\n\", network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DRL bucle de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 complete.\n",
      "Episode 1 complete.\n",
      "Episode 2 complete.\n",
      "Episode 3 complete.\n",
      "Episode 4 complete.\n",
      "Episode 5 complete.\n",
      "Episode 6 complete.\n",
      "Episode 7 complete.\n",
      "Episode 8 complete.\n",
      "Episode 9 complete.\n",
      "Episode 10 complete.\n",
      "Episode 11 complete.\n",
      "Episode 12 complete.\n",
      "Episode 13 complete.\n",
      "Episode 14 complete.\n",
      "Episode 15 complete.\n",
      "Episode 16 complete.\n",
      "Episode 17 complete.\n",
      "Episode 18 complete.\n",
      "Episode 19 complete.\n",
      "Episode 20 complete.\n",
      "Episode 21 complete.\n",
      "Episode 22 complete.\n",
      "Episode 23 complete.\n",
      "Episode 24 complete.\n",
      "Episode 25 complete.\n",
      "Episode 26 complete.\n",
      "Episode 27 complete.\n",
      "Episode 28 complete.\n",
      "Episode 29 complete.\n",
      "Episode 30 complete.\n",
      "Episode 31 complete.\n",
      "Episode 32 complete.\n",
      "Episode 33 complete.\n",
      "Episode 34 complete.\n",
      "Episode 35 complete.\n",
      "Episode 36 complete.\n",
      "Episode 37 complete.\n",
      "Episode 38 complete.\n",
      "Episode 39 complete.\n",
      "Episode 40 complete.\n",
      "Episode 41 complete.\n",
      "Episode 42 complete.\n",
      "Episode 43 complete.\n",
      "Episode 44 complete.\n",
      "Episode 45 complete.\n",
      "Episode 46 complete.\n",
      "Episode 47 complete.\n",
      "Episode 48 complete.\n",
      "Episode 49 complete.\n",
      "Episode 50 complete.\n",
      "Episode 51 complete.\n",
      "Episode 52 complete.\n",
      "Episode 53 complete.\n",
      "Episode 54 complete.\n",
      "Episode 55 complete.\n",
      "Episode 56 complete.\n",
      "Episode 57 complete.\n",
      "Episode 58 complete.\n",
      "Episode 59 complete.\n",
      "Episode 60 complete.\n",
      "Episode 61 complete.\n",
      "Episode 62 complete.\n",
      "Episode 63 complete.\n",
      "Episode 64 complete.\n",
      "Episode 65 complete.\n",
      "Episode 66 complete.\n",
      "Episode 67 complete.\n",
      "Episode 68 complete.\n",
      "Episode 69 complete.\n",
      "Episode 70 complete.\n",
      "Episode 71 complete.\n",
      "Episode 72 complete.\n",
      "Episode 73 complete.\n",
      "Episode 74 complete.\n",
      "Episode 75 complete.\n",
      "Episode 76 complete.\n",
      "Episode 77 complete.\n",
      "Episode 78 complete.\n",
      "Episode 79 complete.\n",
      "Episode 80 complete.\n",
      "Episode 81 complete.\n",
      "Episode 82 complete.\n",
      "Episode 83 complete.\n",
      "Episode 84 complete.\n",
      "Episode 85 complete.\n",
      "Episode 86 complete.\n",
      "Episode 87 complete.\n",
      "Episode 88 complete.\n",
      "Episode 89 complete.\n",
      "Episode 90 complete.\n",
      "Episode 91 complete.\n",
      "Episode 92 complete.\n",
      "Episode 93 complete.\n",
      "Episode 94 complete.\n",
      "Episode 95 complete.\n",
      "Episode 96 complete.\n",
      "Episode 97 complete.\n",
      "Episode 98 complete.\n",
      "Episode 99 complete.\n",
      "Episode 100 complete.\n",
      "Episode 101 complete.\n",
      "Episode 102 complete.\n",
      "Episode 103 complete.\n",
      "Episode 104 complete.\n",
      "Episode 105 complete.\n",
      "Episode 106 complete.\n",
      "Episode 107 complete.\n",
      "Episode 108 complete.\n",
      "Episode 109 complete.\n",
      "Episode 110 complete.\n",
      "Episode 111 complete.\n",
      "Episode 112 complete.\n",
      "Episode 113 complete.\n",
      "Episode 114 complete.\n",
      "Episode 115 complete.\n",
      "Episode 116 complete.\n",
      "Episode 117 complete.\n",
      "Episode 118 complete.\n",
      "Episode 119 complete.\n",
      "Episode 120 complete.\n",
      "Episode 121 complete.\n",
      "Episode 122 complete.\n",
      "Episode 123 complete.\n",
      "Episode 124 complete.\n",
      "Episode 125 complete.\n",
      "Episode 126 complete.\n",
      "Episode 127 complete.\n",
      "Episode 128 complete.\n",
      "Episode 129 complete.\n",
      "Episode 130 complete.\n",
      "Episode 131 complete.\n",
      "Episode 132 complete.\n",
      "Episode 133 complete.\n",
      "Episode 134 complete.\n",
      "Episode 135 complete.\n",
      "Episode 136 complete.\n",
      "Episode 137 complete.\n",
      "Episode 138 complete.\n",
      "Episode 139 complete.\n",
      "Episode 140 complete.\n",
      "Episode 141 complete.\n",
      "Episode 142 complete.\n",
      "Episode 143 complete.\n",
      "Episode 144 complete.\n",
      "Episode 145 complete.\n",
      "Episode 146 complete.\n",
      "Episode 147 complete.\n",
      "Episode 148 complete.\n",
      "Episode 149 complete.\n",
      "Episode 150 complete.\n",
      "Episode 151 complete.\n",
      "Episode 152 complete.\n",
      "Episode 153 complete.\n",
      "Episode 154 complete.\n",
      "Episode 155 complete.\n",
      "Episode 156 complete.\n",
      "Episode 157 complete.\n",
      "Episode 158 complete.\n",
      "Episode 159 complete.\n",
      "Episode 160 complete.\n",
      "Episode 161 complete.\n",
      "Episode 162 complete.\n",
      "Episode 163 complete.\n",
      "Episode 164 complete.\n",
      "Episode 165 complete.\n",
      "Episode 166 complete.\n",
      "Episode 167 complete.\n",
      "Episode 168 complete.\n",
      "Episode 169 complete.\n",
      "Episode 170 complete.\n",
      "Episode 171 complete.\n",
      "Episode 172 complete.\n",
      "Episode 173 complete.\n",
      "Episode 174 complete.\n",
      "Episode 175 complete.\n",
      "Episode 176 complete.\n",
      "Episode 177 complete.\n",
      "Episode 178 complete.\n",
      "Episode 179 complete.\n",
      "Episode 180 complete.\n",
      "Episode 181 complete.\n",
      "Episode 182 complete.\n",
      "Episode 183 complete.\n",
      "Episode 184 complete.\n",
      "Episode 185 complete.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m loss \u001b[38;5;241m=\u001b[39m calculate_loss(network, state, action, next_state, reward, done)\n\u001b[0;32m     46\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 47\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()        \n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Update the state\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\julig\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    523\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\julig\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[0;32m    290\u001b[0m     tensors,\n\u001b[0;32m    291\u001b[0m     grad_tensors_,\n\u001b[0;32m    292\u001b[0m     retain_graph,\n\u001b[0;32m    293\u001b[0m     create_graph,\n\u001b[0;32m    294\u001b[0m     inputs,\n\u001b[0;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    297\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\julig\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    769\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    770\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# generrado por chatgpt\n",
    "def calculate_loss(network, state, action, next_state, reward, done):\n",
    "    # Convertir el estado actual y el siguiente en tensores\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "    next_state_tensor = torch.tensor(next_state, dtype=torch.float32)\n",
    "    \n",
    "    # Pasar el estado actual por la red para obtener los valores Q\n",
    "    q_values = network(state_tensor)\n",
    "    \n",
    "    # Obtener el valor Q para la acción tomada\n",
    "    q_value = q_values[action]\n",
    "    \n",
    "    # Calcular el valor Q objetivo\n",
    "    with torch.no_grad():\n",
    "        next_q_values = network(next_state_tensor)\n",
    "        max_next_q_value = torch.max(next_q_values).item()\n",
    "        target_q_value = reward + (0.99 * max_next_q_value * (not done))\n",
    "    \n",
    "    # Calcular la pérdida\n",
    "    loss = F.mse_loss(q_value, torch.tensor(target_q_value, dtype=torch.float32))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "#generado por chatgpt\n",
    "def select_action(network, state):\n",
    "    # Convertir el estado en un tensor de PyTorch\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "    # Pasar el estado por la red para obtener las probabilidades de acción\n",
    "    with torch.no_grad():  # No necesitamos gradientes durante la inferencia\n",
    "        action_probs = network(state_tensor)\n",
    "    # Seleccionar la acción con la mayor probabilidad\n",
    "    action = torch.argmax(action_probs).item()\n",
    "    return action\n",
    "\n",
    "# Run ten episodes\n",
    "for episode in range(1000):\n",
    "    state, info = env.reset()\n",
    "    done = False    \n",
    "    # Run through steps until done\n",
    "    while not done:\n",
    "        action = select_action(network, state)        \n",
    "        # Take the action\n",
    "        next_state, reward, terminated, truncated, _ = (env.step(action))\n",
    "        done = terminated or truncated        \n",
    "        loss = calculate_loss(network, state, action, next_state, reward, done)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        # Update the state\n",
    "        state = next_state\n",
    "    print(f\"Episode {episode} complete.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
